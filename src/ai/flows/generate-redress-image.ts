'use server';

/**
 * @fileOverview Combines a person image and scene description to generate a new image using Vertex AI SDK.
 *
 * - generateRedressImage - A function that handles the image generation process.
 * - GenerateRedressImageInput - The input type for the generateRedressImage function.
 * - GenerateRedressImageOutput - The return type for the generateRedressImage function.
 */

import {z} from 'zod';
import {v1, helpers} from '@google-cloud/aiplatform';

// Configure the client
const {PredictionServiceClient} = v1;
const clientOptions = {
  apiEndpoint: 'us-central1-aiplatform.googleapis.com',
};
const predictionServiceClient = new PredictionServiceClient(clientOptions);

const GenerateRedressImageInputSchema = z.object({
  personDataUri: z
    .string()
    .describe(
      "A photo of a person, as a data URI that must include a MIME type and use Base64 encoding. Expected format: 'data:<mimetype>;base64,<encoded_data>'."
    ),
  clothingDataUri: z
    .string()
    .describe(
      "A photo of a garment or clothes, as a data URI that must include a MIME type and use Base64 encoding. Expected format: 'data:<mimetype>;base64,<encoded_data>'."
    ),
});
export type GenerateRedressImageInput = z.infer<typeof GenerateRedressImageInputSchema>;

const GenerateRedressImageOutputSchema = z.object({
  generatedImageDataUri: z
    .string()
    .describe(
      "The generated image, as a data URI that must include a MIME type and use Base64 encoding. Expected format: 'data:<mimetype>;base64,<encoded_data>'"
    ),
});
export type GenerateRedressImageOutput = z.infer<
  typeof GenerateRedressImageOutputSchema
>;

export async function generateRedressImage(
  input: GenerateRedressImageInput
): Promise<GenerateRedressImageOutput> {
  // Use the client to automatically discover the project ID
  const projectId = process.env.GCP_PROJECT || await predictionServiceClient.getProjectId();
  const location = 'us-central1';

  const endpoint = `projects/${projectId}/locations/${location}/publishers/google/models/virtual-try-on-preview-08-04`;

  const personImageMimeType = input.personDataUri.split(';')[0].split(':')[1];
  const personImageBase64 = input.personDataUri.split(',')[1];

  const clothingImageMimeType = input.clothingDataUri.split(';')[0].split(':')[1];
  const clothingImageBase64 = input.clothingDataUri.split(',')[1];

  const instance = {
    personImage: {"image": {
      bytesBase64Encoded: personImageBase64,
      mimeType: personImageMimeType,
    }},
    productImages: [{
      "image": {
        bytesBase64Encoded: clothingImageBase64,
        mimeType: clothingImageMimeType,
      }
    }]
  };

  const instances = [helpers.toValue(instance)];

  const parameters = helpers.toValue({
    sampleCount: 1,
  });

  const request = {
    endpoint,
    instances,
    parameters,
  };

  try {
    const [response] = await predictionServiceClient.predict(request);

    if (!response.predictions || response.predictions.length === 0) {
      throw new Error('No image was generated by Vertex AI.');
    }

    const prediction = helpers.fromValue(response.predictions[0] as any);

    if (!prediction || !prediction.bytesBase64Encoded) {
      throw new Error('No image data found in Vertex AI response.');
    }

    const generatedImageDataUri = `data:image/png;base64,${prediction.bytesBase64Encoded}`;

    return {generatedImageDataUri};
  } catch (error) {
    console.error('Vertex AI Prediction Error:', error);
    throw new Error('Failed to generate image with Vertex AI.');
  }
}
