'use server';

/**
 * @fileOverview Generates a video from an image using a Vertex AI model.
 *
 * - generateVideo - A function that handles the video generation.
 * - GenerateVideoInput - The input type for the generateVideo function.
 * - GenerateVideoOutput - The return type for the generateVideo function.
 */

import {z} from 'zod';
import {v1, helpers} from '@google-cloud/aiplatform';

// Configure the client
const {PredictionServiceClient} = v1;
const clientOptions = {
  apiEndpoint: 'us-central1-aiplatform.googleapis.com',
};
const predictionServiceClient = new PredictionServiceClient(clientOptions);

const GenerateVideoInputSchema = z.object({
  imageDataUri: z
    .string()
    .describe(
      "A photo, as a data URI that must include a MIME type and use Base64 encoding. Expected format: 'data:<mimetype>;base64,<encoded_data>'."
    ),
});
export type GenerateVideoInput = z.infer<typeof GenerateVideoInputSchema>;

const GenerateVideoOutputSchema = z.object({
  videoDataUri: z
    .string()
    .describe(
      "The generated video, as a data URI that must include a MIME type and use Base64 encoding. Expected format: 'data:video/mp4;base64,<encoded_data>'"
    ),
});
export type GenerateVideoOutput = z.infer<typeof GenerateVideoOutputSchema>;

export async function generateVideo(
  input: GenerateVideoInput
): Promise<GenerateVideoOutput> {
  const projectId = process.env.GCP_PROJECT || await predictionServiceClient.getProjectId();
  const location = 'us-central1';

  // Using a video generation model.
  const endpoint = `projects/${projectId}/locations/${location}/publishers/google/models/imagen-3.0-video-generate-preview-05-30`;

  const imageMimeType = input.imageDataUri.split(';')[0].split(':')[1];
  const imageBase64 = input.imageDataUri.split(',')[1];

  const instance = {
    prompt: "Animate this image with subtle motion. The background can have a gentle breeze effect, and the person's clothes and hair can sway slightly. The person's expression should remain neutral.",
    referenceImages: [
      {
        referenceImage: {
          bytesBase64Encoded: imageBase64,
          mimeType: imageMimeType,
        },
      },
    ],
  };

  const instances = [helpers.toValue(instance)];

  const parameters = helpers.toValue({
    sampleCount: 1,
    videoLength: "4", // 4 seconds
    fps: 24,
  });

  const request = {
    endpoint,
    instances,
    parameters,
  };

  try {
    const [response] = await predictionServiceClient.predict(request);

    if (!response.predictions || response.predictions.length === 0) {
      throw new Error('No video was generated by Vertex AI.');
    }

    const prediction = helpers.fromValue(response.predictions[0] as any);

    if (!prediction || !prediction.bytesBase64Encoded) {
      throw new Error('No video data found in Vertex AI response.');
    }

    const videoDataUri = `data:video/mp4;base64,${prediction.bytesBase64Encoded}`;

    return {videoDataUri};
  } catch (error) {
    console.error('Vertex AI Video Prediction Error:', error);
    throw new Error('Failed to generate video with Vertex AI.');
  }
}
